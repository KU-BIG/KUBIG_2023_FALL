{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "print(torch.__version__)from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "import io\n",
        "import os\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "#import boto3\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "#from botocore import UNSIGNED\n",
        "#from botocore.config import Config\n",
        "from IPython.display import Audio\n",
        "from torchaudio.utils import download_asset\n",
        "\n",
        "!pip install modelscope\n",
        "!pip install funasr\n",
        "\n",
        "# sample rate\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "# told\n",
        "from modelscope.pipelines import pipeline\n",
        "from modelscope.utils.constant import Tasks"
      ],
      "metadata": {
        "id": "3Ih6SZLtNA9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnQcAp_SM8BB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOLD 샘플 예제\n",
        "\n",
        "# initialize pipeline\n",
        "inference_diar_pipline = pipeline(\n",
        "    mode=\"sond_demo\",\n",
        "    num_workers=0,\n",
        "    task=Tasks.speaker_diarization,\n",
        "    diar_model_config=\"sond.yaml\",\n",
        "    model='damo/speech_diarization_sond-zh-cn-alimeeting-16k-n16k4-pytorch',\n",
        "    reversion=\"v1.0.5\",\n",
        "    sv_model=\"damo/speech_xvector_sv-zh-cn-cnceleb-16k-spk3465-pytorch\",\n",
        "    sv_model_revision=\"v1.2.2\",\n",
        ")\n",
        "\n",
        "# input: a list of audio in which the first item is a speech recording to detect speakers,\n",
        "# and the following wav file are used to extract speaker embeddings.\n",
        "audio_list = [\n",
        "    \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_data/speaker_diarization/record.wav\",\n",
        "    \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_data/speaker_diarization/spk1.wav\",\n",
        "    \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_data/speaker_diarization/spk2.wav\",\n",
        "    \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_data/speaker_diarization/spk3.wav\",\n",
        "    \"https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_data/speaker_diarization/spk4.wav\",\n",
        "]\n",
        "\n",
        "results = inference_diar_pipline(audio_in=audio_list)\n",
        "\n",
        "# output\n",
        "print(results)"
      ],
      "metadata": {
        "id": "J7Y0VlLpND0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output\n",
        "'''\n",
        "{'text': '\n",
        "spk1 [(0.0, 8.88), (10.56, 15.2)]\\n\n",
        "spk2 [(8.88, 9.76)]\\n\n",
        "spk3 [(9.6, 11.04), (15.12, 15.68)]\\n\n",
        "spk4 [(11.2, 11.76)]'}\n",
        "'''"
      ],
      "metadata": {
        "id": "vj7SST5yNI4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 12월 침착맨 통닭천사 테스트"
      ],
      "metadata": {
        "id": "EtLuRGaLNM2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 데이터 EDA\n",
        "\n",
        "# 기본정보 확인해보기\n",
        "sample_paper_wav = download_asset(\"/content/drive/MyDrive/23-2_kubig_contest/TOLD_test_12_28/norm_chimtongshorts.wav\")\n",
        "print(torchaudio.info(sample_paper_wav))\n",
        "\n",
        "# 침통 파일 : AudioMetaData(sample_rate=8000, num_frames=22052, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n",
        "# sample rate 변환 : AudioMetaData(sample_rate=16000, num_frames=44104, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n",
        "# 원본_record : AudioMetaData(sample_rate=16000, num_frames=256000, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n",
        "# 원본_spk2 : AudioMetaData(sample_rate=16000, num_frames=153760, num_channels=1, bits_per_sample=16, encoding=PCM_S)\n"
      ],
      "metadata": {
        "id": "jS23LKmKNz5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 전처리1 : sample rate 변환 -> audicity로 진행\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/23-2_kubig_contest/TOLD_test_12_28/chimtongshorts_chim.wav'\n",
        "# # WAV 파일 로드 (현재 sample rate: 8000)\n",
        "# audio_data, sr = librosa.load(file_path, sr=8000)\n",
        "# # 변경할 샘플 레이트 설정\n",
        "# desired_sr = 16000\n",
        "# resampled_data = librosa.resample(y=audio_data,orig_sr=44100, target_sr=16000)\n",
        "# print(\"새로운 sample rate:\", desired_sr)\n",
        "\n",
        "# new_file_path = '/content/drive/MyDrive/23-2_kubig_contest/TOLD_test_12_28/new_chimtongshorts_chim.wav'\n",
        "# sf.write(new_file_path, resampled_data, desired_sr)\n"
      ],
      "metadata": {
        "id": "K5cYmpQMNNC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 전처리2 : stereo to mono -> audicity로 진행"
      ],
      "metadata": {
        "id": "hFqkw3QlNmJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 침착맨, 통천 입력\n",
        "\n",
        "audio_list = [\n",
        "    \"/content/drive/MyDrive/23-2_kubig_contest/TOLD_test_12_28/norm_chimtongshorts.wav\", # 전체\n",
        "    \"/content/drive/MyDrive/23-2_kubig_contest/TOLD_test_12_28/norm_chimtongshorts-tong.wav\", # 침착맨\n",
        "    \"/content/drive/MyDrive/23-2_kubig_contest/TOLD_test_12_28/norm_chimtongshorts_chim.wav\", # 통천\n",
        "]\n",
        "\n",
        "results = inference_diar_pipline(audio_in=audio_list)\n",
        "print(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "YZBqsTNAN_vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시각화\n",
        "\n",
        "def plot_waveform(waveform, sample_rate):\n",
        "  waveform_np = waveform.numpy()\n",
        "  num_channels, num_frames = waveform_np.shape\n",
        "  figures, axes = plt.subplots(num_channels, 1)\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  if num_channels==1:\n",
        "    axes = [axes]\n",
        "\n",
        "  for c in range(num_channels):\n",
        "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "    axes[c].grid(True)\n",
        "\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'channel {c+1}')\n",
        "\n",
        "sample_wav = \"/content/drive/MyDrive/23-2_kubig_contest/TOLD_test_12_28/norm_chimtongshorts.wav\"\n",
        "waveform, sample_rate = torchaudio.load(sample_wav)\n",
        "plot_waveform(waveform, sample_rate)"
      ],
      "metadata": {
        "id": "b41bdjqmOGYR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}