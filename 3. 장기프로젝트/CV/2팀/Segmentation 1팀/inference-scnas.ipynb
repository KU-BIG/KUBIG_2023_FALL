{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import VolumeDataset\n",
    "from scnas import ScNas\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed          = 42\n",
    "    debug         = False # set debug=False for Full Training\n",
    "    exp_name      = 'baseline'\n",
    "    output_dir    = './checkpoint/'\n",
    "    model_name    = 'scnas'\n",
    "\n",
    "    valid_bs      = 8\n",
    "    volume_shape  = [128, 128, 128]\n",
    "    overlap_size  = 64 \n",
    "\n",
    "    device        = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    gt_df = \"../data/gt.csv\"\n",
    "    data_root = \"../data/blood-vessel-segmentation/\"\n",
    "\n",
    "    valid_groups = [\"train/kidney_1_dense\", \"train/kidney_2\"]\n",
    "    #valid_groups = [\"test/kidney_5\", \"test/kidney_6\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found images: 0\n"
     ]
    }
   ],
   "source": [
    "DATASET_FOLDER = \"/kaggle/input/blood-vessel-segmentation\"\n",
    "ls_images = glob(os.path.join(DATASET_FOLDER, \"test\", \"*\", \"*\", \"*.tif\"))\n",
    "print(f\"found images: {len(ls_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2279 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2279/2279 [00:14<00:00, 158.60it/s]\n"
     ]
    }
   ],
   "source": [
    "inference_path = [os.path.join(CFG.data_root, group) for group in CFG.valid_groups]\n",
    "\n",
    "kidney_5 = VolumeDataset(inference_path[0], subvol_size=CFG.volume_shape[0], overlap=CFG.overlap_size)\n",
    "#kidney_6 = VolumeDataset(inference_path[1], subvol_size=CFG.volume_shape[0], overlap=CFG.overlap_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_segmentation(model, dataset, subvol_size, overlap, batch_size, device):\n",
    "    \"\"\"\n",
    "    Perform batched inference on subvolumes and reconstruct the segmented volume.\n",
    "\n",
    "    Parameters:z\n",
    "    model (torch.nn.Module): Trained PyTorch model for segmentation.\n",
    "    dataset (VolumeDataset): The dataset containing the subvolumes.\n",
    "    subvol_size (int): Size of the subvolumes.\n",
    "    overlap (int): Size of the overlap between subvolumes.\n",
    "    batch_size (int): The size of each batch.\n",
    "    device (str): Device to perform inference on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The reconstructed segmented volume.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    C, Z, Y, X = dataset.volume.shape\n",
    "    segmented_volume = np.zeros_like(dataset.volume, dtype=np.float32)\n",
    "    count_volume = np.ones_like(dataset.volume, dtype=np.int32)  # Count for averaging\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, coords in tqdm(loader, desc='Segmenting', unit='batch'):\n",
    "            batch = batch.to(device)  # Send batch to device\n",
    "            segmented_batch = torch.sigmoid(model(batch)).cpu().numpy()  # Perform inference and send to cpu\n",
    "            z_coords, y_coords, x_coords = coords\n",
    "\n",
    "            # Iterate through each subvolume in the batch\n",
    "            for idx, segmented_subvol in enumerate(segmented_batch):  # Unpack coordinates\n",
    "                z = z_coords[idx].item()  # Convert tensor to integer\n",
    "                y = y_coords[idx].item()\n",
    "                x = x_coords[idx].item()\n",
    "                \n",
    "                segmented_volume[:, z:z+subvol_size, y:y+subvol_size, x:x+subvol_size] += segmented_subvol\n",
    "                count_volume[:, z:z+subvol_size, y:y+subvol_size, x:x+subvol_size] += 1\n",
    "\n",
    "    segmented_volume /= count_volume\n",
    "\n",
    "    return segmented_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(device):\n",
    "    model = ScNas(num_feature = 8, num_layers = 6, num_multiplier = 2, num_classes = 1, use_bridge = True, input_channel = 1)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device, path):\n",
    "    model = build_model(device)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(CFG.device, \"./checkpoint/last_epoch.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting: 100%|██████████| 1050/1050 [13:10<00:00,  1.33batch/s]\n"
     ]
    }
   ],
   "source": [
    "kidney_5_mask = infer_segmentation(model, \n",
    "                                   dataset = kidney_5, \n",
    "                                   subvol_size=CFG.volume_shape[0], \n",
    "                                   overlap=CFG.overlap_size, \n",
    "                                   batch_size=CFG.valid_bs, \n",
    "                                   device=CFG.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kidney_6_mask = infer_segmentation(model, \n",
    "                                   dataset = kidney_6, \n",
    "                                   subvol_size=CFG.volume_shape[0], \n",
    "                                   overlap=CFG.overlap_size, \n",
    "                                   batch_size=CFG.valid_bs, \n",
    "                                   device=CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(kidney_5_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "preds.append((kidney_5_mask>0.5).astype(np.uint8))\n",
    "#preds.append((kidney_6_mask>0.5).astype(np.uint8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    if rle=='':\n",
    "        rle = '1 0'\n",
    "    return rle\n",
    "\n",
    "def remove_small_objects(img, min_size):\n",
    "    # Find all connected components (labels)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n",
    "\n",
    "    # Create a mask where small objects are removed\n",
    "    new_img = np.zeros_like(img)\n",
    "    for label in range(1, num_labels):\n",
    "        if stats[label, cv2.CC_STAT_AREA] >= min_size:\n",
    "            new_img[labels == label] = 1\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rles = []\n",
    "\n",
    "for pred in preds:\n",
    "    pred = pred.squeeze()\n",
    "    for pred_by_slice in pred:\n",
    "        pred_by_slice = remove_small_objects(pred_by_slice, 10)\n",
    "        rle = rle_encode(pred_by_slice)\n",
    "        rles.append(rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "ids = []\n",
    "for p_img in tqdm(ls_images):\n",
    "    path_ = p_img.split(os.path.sep)\n",
    "    # parse the submission ID\n",
    "    dataset = path_[-3]\n",
    "    slice_id, _ = os.path.splitext(path_[-1])\n",
    "    ids.append(f\"{dataset}_{slice_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    \"id\": ids,\n",
    "    \"rle\": rles\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
