{"cells":[{"cell_type":"markdown","source":["#3ì£¼ì°¨ ê³¼ì œ ì„¤ëª…\n","<p>ì´ë²ˆ ê³¼ì œì˜ ëª©í‘œëŠ” ì„¸ì…˜ ì‹œê°„ì— ê°œë…ìœ¼ë¡œ ë°°ìš´ Visualizationì„ ì§ì ‘ ì‹¤ìŠµí•´ë³´ë©´ì„œ CNNì•ˆì—ì„œëŠ” ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€, Gradient ascentê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ” ê²ƒì¸ì§€ ì—¬ëŸ¬ë¶„ë“¤ì´ ëª¸ì†Œ ëŠë¼ì‹œëŠ” ê²ƒì…ë‹ˆë‹¤.</p>\n","\n","<p>í‰ì†Œì— í•˜ì…¨ë˜ ì–´ë–¤ ëª¨ë¸ êµ¬í˜„ì´ë‚˜, í•™ìŠµì„ ì‹œí‚¤ëŠ” ê³¼ì •ì€ ì•„ë‹ˆì§€ë§Œ ë”¥ëŸ¬ë‹ì—ê²Œ ë§¤ë²ˆ ì£¼ì–´ì§€ëŠ” ê·¼ë³¸ì ì¸ ê³¼ì œì¸ \"ë„ëŒ€ì²´ ì–˜ë„¤ëŠ” ë¬´ì—‡ì„, ì–´ë–»ê²Œ í•˜ê³  ìˆëŠ” ê²ƒì´ëƒ\"ì— ëŒ€í•œ ìê·¸ë§ˆí•œ ì²´í—˜ì´ì‹œë¼ê³  ìƒê°í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤ ğŸ˜‚</p>\n","\n","<p> ì£¼ì–´ì§„ ê³¼ì œëŠ” ì´ 3ê°œì´ë©° í›Œë¥­í•œ êµë³´ì¬ì¸ CS231 ê³µì‹ assignment 3ì„ ëŒ€ë‹¤ìˆ˜ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤ ğŸ™‡ğŸ»â€â™‚ï¸</p>"],"metadata":{"id":"Gv1pJdQ7MYKu"}},{"cell_type":"markdown","source":["# Drive Mount & Download COCO"],"metadata":{"id":"2yolex4ENuq7"}},{"cell_type":"code","source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"JZsvCiL8VSbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCNCEFN99DNF"},"outputs":[],"source":["# ë³¸ì¸ì˜ ë‹¤ìš´ë¡œë“œë°›ì€ í´ë”ê²½ë¡œì— ë§ê²Œ ì„¤ì •í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n","FOLDERNAME = \"/2023_summer_kubig\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# COCO datasetì„ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤\n","# if it doesn't already exist.\n","%cd /content/drive/My\\ Drive/$FOLDERNAME\n","!bash get_imagenet_val.sh\n","%cd /content/drive/My\\ Drive/$FOLDERNAME"]},{"cell_type":"markdown","metadata":{"collapsed":true,"tags":["pdf-title"],"id":"APaGwyY29DNX"},"source":["# Network Visualization\n","\n","\n","\n","*   Image gradients (Gradient ascent)ë¥¼ í™œìš©í•´ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•´ë´…ë‹ˆë‹¤\n","*   Pretrainedëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³ , loss functionì„ ëª©ì ì— ë§ê²Œ ì§€ì •í•˜ì—¬ ì´ë¯¸ì§€ í”½ì…€ê³¼ lossê°„ì˜ gradientë¥¼ backpropagationì„ í†µí•´ ê³„ì‚°í•©ë‹ˆë‹¤\n","\n","*   ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •í•˜ê³ , lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ í•©ì„±í•˜ê¸° ìœ„í•´ Gradient descent on the image (Gradient ascent)ë¥¼ ì ìš©í•©ë‹ˆë‹¤\n","\n","<br>\n","\n","<h1>Image Generationì„ ìœ„í•œ 3ê°€ì§€ techniqueì„ í•™ìŠµí•©ë‹ˆë‹¤</h1>\n","\n","\n","1.   Saliency Maps : ì–´ë–¤ ì´ë¯¸ì§€ í”½ì…€ì´ classification decisionì— ì˜í–¥ì„ ì£¼ëŠ”ê°€?\n","2.   Fooling images: pretrained networkë¥¼ êµë€ì‹œí‚¬ ìˆ˜ ìˆëŠ” imageë¥¼ ë§Œë“¤ì–´ë³´ì\n","3. Class Visualization : íŠ¹ì • í´ë˜ìŠ¤ì˜ classification scoreë¥¼ ìµœëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ë¥¼ í•©ì„±í•´ë³´ì"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"7YDFZ9g_9DNf"},"outputs":[],"source":["# Setup cell.\n","import torch\n","import torchvision\n","import torchvision.transforms as T\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","import os\n","from scipy.ndimage import gaussian_filter1d\n","from PIL import Image\n","\n","SQUEEZENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","SQUEEZENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # Set default size of plots.\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","source":["## Help Function"],"metadata":{"id":"ZzBXmGCKKxau"}},{"cell_type":"markdown","source":["\n","\n","<b>ì‚¬ìš©í•  Pretrained modelì´ ì´ë¯¸ í‘œì¤€í™”ëœ ì´ë¯¸ì§€ì— ëŒ€í•´ í•™ìŠµë¼ì„œ, ê³¼ì œì˜ í¸ì˜ë¥¼ ìœ„í•´ Model inferenceì— í•„ìš”í•œ ì „ì²˜ë¦¬ ê´€ë ¨ í•¨ìˆ˜ë“¤ì„ ë¯¸ë¦¬ ëª¨ì•„ë†“ì•˜ìŠµë‹ˆë‹¤.</b>\n","\n"],"metadata":{"id":"ZrQqot9XP-JP"}},{"cell_type":"code","source":["def preprocess(img, size=224): #Standard-scaling\n","    transform = T.Compose([\n","        T.Resize(size),\n","        T.ToTensor(),\n","        T.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n","                    std=SQUEEZENET_STD.tolist()),\n","        T.Lambda(lambda x: x[None]),\n","    ])\n","    return transform(img)\n","\n","def deprocess(img, should_rescale=True):\n","    transform = T.Compose([\n","        T.Lambda(lambda x: x[0]),\n","        T.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n","        T.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n","        T.Lambda(rescale) if should_rescale else T.Lambda(lambda x: x),\n","        T.ToPILImage(),\n","    ])\n","    return transform(img)\n","\n","def rescale(x): #Minmax-scaling\n","    low, high = x.min(), x.max()\n","    x_rescaled = (x - low) / (high - low)\n","    return x_rescaled\n","\n","def blur_image(X, sigma=1): #Gaussian_filter\n","    X_np = X.cpu().clone().numpy()\n","    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n","    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n","    X.copy_(torch.Tensor(X_np).type_as(X))\n","    return X\n","\n","def jitter(X, ox, oy): #pixel change\n","    \"\"\"\n","    Helper function to randomly jitter an image.\n","\n","    Inputs\n","    - X: PyTorch Tensor of shape (N, C, H, W)\n","    - ox, oy: Integers giving number of pixels to jitter along W and H axes\n","\n","    Returns: A new PyTorch Tensor of shape (N, C, H, W)\n","    \"\"\"\n","    if ox != 0:\n","        left = X[:, :, :, :-ox]\n","        right = X[:, :, :, -ox:]\n","        X = torch.cat([right, left], dim=3)\n","    if oy != 0:\n","        top = X[:, :, :-oy]\n","        bottom = X[:, :, -oy:]\n","        X = torch.cat([bottom, top], dim=2)\n","    return X\n"],"metadata":{"id":"oR4TvAXq4sOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Imagenet_valid ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤ <br>\n","ë°‘ì— ë‹¤ìš´ë¡œë“œ ë°›ì„ íŒŒì¼ ê²½ë¡œë§Œ ìˆ˜ì •í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤!"],"metadata":{"id":"G97IbCCE8KNl"}},{"cell_type":"code","source":["def load_imagenet_val(num=None):\n","    \"\"\"Load a handful of validation images from ImageNet.\n","\n","    Inputs:\n","    - num: Number of images to load (max of 25)\n","\n","    Returns:\n","    - X: numpy array with shape [num, 224, 224, 3]\n","    - y: numpy array of integer image labels, shape [num]\n","    - class_names: dict mapping integer label to class name\n","    \"\"\"\n","    imagenet_fn = os.path.join(\"imagenet_val_25.npz\")\n","    if not os.path.isfile(imagenet_fn):\n","        print(\"file %s not found\" % imagenet_fn)\n","        print(\"Run the following:\")\n","        print(\"\")\n","        print(\"bash get_imagenet_val.sh\")\n","        assert False, \"Need to download imagenet_val_25.npz\"\n","\n","    # modify the default parameters of np.load\n","    # https://stackoverflow.com/questions/55890813/how-to-fix-object-arrays-cannot-be-loaded-when-allow-pickle-false-for-imdb-loa\n","    np_load_old = np.load\n","    np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n","    f = np.load(imagenet_fn)\n","    np.load = np_load_old\n","    X = f[\"X\"]\n","    y = f[\"y\"]\n","    class_names = f[\"label_map\"].item()\n","    if num is not None:\n","        X = X[:num]\n","        y = y[:num]\n","    return X, y, class_names"],"metadata":{"id":"7B8StQok8JPm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6SS57rU89DNm"},"source":["# Pretrained Model\n","\n","## Image Generation í•­ëª©ì—ì„œëŠ” ImageNetì—ì„œ pretrainedí•œ modelì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n","\n","\n","*   íŠ¹íˆ ì—¬ê¸°ì„œëŠ” Squeezenetì´ë¼ëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ”ë° AlexNetê³¼ ìœ ì‚¬í•˜ì§€ë§Œ í›¨ì”¬ ë” ê°€ë³ê³  ë¹ ë¥¸ ëª¨ë¸ì…ë‹ˆë‹¤\n","*   CPUë¡œë„ Image Generationì´ ê°€ëŠ¥í•©ë‹ˆë‹¤\n","\n","\n","[1] Iandola et al, \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size\", arXiv 2016"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znVNVJDx9DNo"},"outputs":[],"source":["# Download and load the pretrained SqueezeNet model.\n","model = torchvision.models.squeezenet1_1(pretrained=True)\n","\n","# Gradient ê³„ì‚° X\n","for param in model.parameters():\n","    param.requires_grad = False"]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"ngQ6H9W59DNq"},"source":["## Loading ImageNet Validation Images\n","\n","\n","*   ImageNetì˜ validation set image sampleë“¤ì„ ì‚´í´ë´…ì‹œë‹¤\n","*   Valid setì´ë¯€ë¡œ Pretrained modelì´ í•™ìŠµí•˜ì§€ ì•Šì€ ê²ƒ"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"IRuEfjXv9DNt"},"outputs":[],"source":["X, y, class_names = load_imagenet_val(num=5)\n","\n","plt.figure(figsize=(12, 6))\n","for i in range(5):\n","    plt.subplot(1, 5, i + 1)\n","    plt.imshow(X[i])\n","    plt.title(class_names[y[i]])\n","    plt.axis('off')\n","plt.gcf().tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"7ebdpAPV9DNu"},"source":["# Saliency Maps : ì–´ë–¤ ì´ë¯¸ì§€ í”½ì…€ì´ classification decisionì— ì˜í–¥ì„ ì£¼ëŠ”ê°€?\n","\n","\n","\n","\n","1.  ì •ë‹µ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” unnormalized scoreì˜ gradientë¥¼ ì´ë¯¸ì§€ì˜ í”½ì…€ì— ëŒ€í•´ ê³„ì‚°í•©ë‹ˆë‹¤.\n","2.  Image Shape : (3,H,W) -> Gradient Shape : (3,H,W)\n","3.  ê° í”½ì…€ì— ëŒ€í•˜ì—¬, gradientëŠ” í”½ì…€ê°’ì´ ë°”ë€”ë•Œë§ˆë‹¤ classification scoreê°€ ì–¼ë§Œí¼ ë³€í• ì§€ë¥¼ ì˜ë¯¸\n","4. Gradientì˜ ì ˆëŒ“ê°’ ê³„ì‚° -> 3 channelì— ëŒ€í•œ maximum valueë§Œ (H,W) saliency mapì— ë³´ì¡´ (ëª¨ë“  ê°’ë“¤ì€ ìŒìˆ˜ê°€ ì•„ë‹™ë‹ˆë‹¤.)\n","\n","[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n","Image Classification Models and Saliency Maps\", ICLR Workshop 2014."]},{"cell_type":"markdown","metadata":{"tags":["pdf-ignore"],"id":"0tZAS1yp9DNw"},"source":["### Hint: PyTorch `gather` method\n","\n","*   `s` = PyTorch Tensor of shape `(N, C)` : 2d array\n","*   `y` = PyTorch Tensor of shape `(N,)` : 1d array containing longs in the range `0 <= y[i] < C`\n","\n","ê·¸ëŸ¬ë©´,\n","\n","`s.gather(1, y.view(-1, 1)).squeeze()`\n","\n","*   `s.gather(1, y.view(-1, 1)).squeeze()` : ê¸°ì¡´ `(N, C)` 2ì°¨ì› ë°°ì—´ì—ì„œ yì˜ ì¸ë±ìŠ¤ì— ë”°ë¼ í–‰ë³„ë¡œ í•˜ë‚˜ì˜ ì›ì†Œë§Œì„ ì„ íƒí•œ `(N,)` 1ì°¨ì› ë°°ì—´ì´ ëœë‹¤\n","*   `s.gather(dim: indexí•˜ê³  ì‹¶ì€ ì¶•, index: indexì— í•„ìš”í•œ ì›ì†Œ)`\n","\n","\n","*   `s.squeeze()` : sizeê°€ 1ì¸ ì°¨ì› ì¶•ì†Œ\n","\n","\n","https://pytorch.org/docs/stable/generated/torch.squeeze<br>\n","https://pytorch.org/docs/stable/generated/torch.gather"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["pdf-ignore"],"id":"b4s5AFdm9DNz"},"outputs":[],"source":["# Example of using gather to select one entry from each row in PyTorch\n","def gather_example():\n","    N, C = 4, 5\n","    s = torch.randn(N, C)\n","    y = torch.LongTensor([1, 2, 1, 3])\n","    print(s)\n","    print(y)\n","    print(s.gather(1, y.view(-1, 1)).squeeze())\n","gather_example()"]},{"cell_type":"markdown","metadata":{"id":"22HHkhYW9DN0"},"source":["#ê³¼ì œ 1 : Compute Salinecy Maps í•¨ìˆ˜ ì§œê¸°"]},{"cell_type":"markdown","source":["* *Saliency Map: ì´ë¯¸ì§€ ë°ì´í„° í”½ì…€ ì¤‘ ì–´ë–¤ í”½ì…€ì´ classificationì— í° ì˜í–¥ì„ ì£¼ëŠ”ì§€*\n","* *ê·¸ ì¤‘ SOD ë°©ì‹: ì´ë¯¸ì§€ ë‚´ì—ì„œ ì‚¬ëŒì´ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•  ë¬¼ì²´/ì§€ì—­ì„ ê²€ì¶œ*"],"metadata":{"id":"TRKbp9qbn1Dv"}},{"cell_type":"code","source":["def compute_saliency_maps(X, y, model):\n","    \"\"\"\n","    Compute a class saliency map using the model for images X and labels y.\n","\n","    Input:\n","    - X: Input images; Tensor of shape (N, 3, H, W)\n","    - y: Labels for X; LongTensor of shape (N,)\n","    - model: A pretrained CNN\n","\n","    Returns:\n","    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n","    images.\n","    \"\"\"\n","    # Make sure the model is in \"test\" mode\n","\n","    model.eval()\n","\n","    # Make input tensor require gradient\n","\n","    X.requires_grad_()\n","\n","    saliency = None\n","\n","    ##############################################################################\n","    # TODO: êµ³ì´ scratchë¡œ êµ¬í˜„ì•ˆí•˜ì…”ë„ ë˜ê³  pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ ì“°ì‹œë©´ ë©ë‹ˆë‹¤.\n","\n","    # ë¹ˆì¹¸ ì‘ì„± 3ê°€ì§€\n","    # 1. Forward pass êµ¬í˜„ : compute the loss over the correct class score\n","    # 2. Backward pass êµ¬í˜„ : compute gradient of the correct class score\n","    # 3. saliency êµ¬í˜„\n","    ##############################################################################\n","    #1.Forward pass : softmaxë¥¼ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤ -> unnormalize score\n","    #Hint: Lossë¥¼ ê³„ì‚°í•˜ë ¤ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ Labelê°’ì„ ì´ìš©í•´ì•¼ê²Ÿì£ ?\n","    #ì¶”ê°€ì ìœ¼ë¡œ Batchë‚´ element lossë¥¼ ë‹¤ í•©ì³ì„œ lossë¥¼ ê²°í•©í•´ì£¼ì„¸ìš”\n","\n","    scores = model(X)\n","\n","    loss = torch.sum(scores.gather(1,y.view(-1,1)).squeeze())\n","\n","    #2.backward pass\n","\n","    loss.backward()\n","\n","    #3.ìš°ë¦¬ê°€ ì›í•˜ëŠ” saliencyëŠ” ì…ë ¥ ì´ë¯¸ì§€ ê°ê°ì— ê´€í•œ class score gradient\n","    #ê·¸ë˜ë””ì–¸íŠ¸ì— ì ˆëŒ“ê°’ì„ ì”Œìš°ê³ , channelì— ëŒ€í•´ max valueê°’ì„ ì·¨í•´ì£¼ì„¸ìš”\n","\n","    saliency = X.grad.abs()\n","\n","    saliency,_ = torch.max(saliency,1)\n","\n","    ##############################################################################\n","    #                             END OF YOUR CODE                               #\n","    ##############################################################################\n","\n","    return saliency"],"metadata":{"id":"loUCD2pEZGm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ìœ„ì—ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ saliency_mapì„ visualizationí•´ë³´ì„¸ìš”"],"metadata":{"id":"Bt6fUpug8hy6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s7q_yt7U9DN5"},"outputs":[],"source":["def show_saliency_maps(X, y):\n","    # Convert X and y from numpy arrays to Torch Tensors\n","    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","    y_tensor = torch.LongTensor(y)\n","\n","    # ì •ë‹µ í´ë˜ìŠ¤ì— ëŒ€í•´ì„œë§Œ saliency_mapì„ ì ìš©\n","    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n","\n","    # Convert the saliency map from Torch Tensor to numpy array and show images\n","    # and saliency maps together.\n","    saliency = saliency.numpy()\n","    N = X.shape[0]\n","    for i in range(N):\n","        plt.subplot(2, N, i + 1)\n","        plt.imshow(X[i])\n","        plt.axis('off')\n","        plt.title(class_names[y[i]])\n","        plt.subplot(2, N, N + i + 1)\n","        plt.imshow(saliency[i], cmap=plt.cm.hot)\n","        plt.axis('off')\n","        plt.gcf().set_size_inches(12, 5)\n","    plt.show()\n","\n","show_saliency_maps(X, y)"]},{"cell_type":"markdown","metadata":{"id":"E6iecIy_9DN7"},"source":["# Fooling Images : pretrained networkë¥¼ êµë€ì‹œí‚¬ ìˆ˜ ìˆëŠ” imageë¥¼ ë§Œë“¤ì–´ë³´ì\n","\n","1.   Imageì™€ target classê°€ ì£¼ì–´ì¡Œì„ë•Œ, target classì˜ classification scoreë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ gradient ascentë¥¼ ì‚¬ìš©í•´ imageë¥¼ updateí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n","2.   Networkê°€ ì‹¤ì œë¡œ ì´ë¯¸ì§€ë¥¼ target classë¡œ ë¶„ë¥˜í•˜ë©´ updateë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤\n","\n","[3] Szegedy et al, \"Intriguing properties of neural networks\", ICLR 2014\n","\n","Implement ```make_fooling_image``` function inside ```cs231n/net_visualization_pytorch.py```"]},{"cell_type":"markdown","source":["* *fooling images: ì‹¤ì œ ì •ë‹µ classê°€ ì•„ë‹Œ ë‹¤ë¥¸ target classë¥¼ ì„¤ì •í•œ í›„, í•´ë‹¹ ì´ë¯¸ì§€ê°€ target classë¡œ ë¶„ë¥˜ë  ë•Œê¹Œì§€ ë°˜ë³µ.*"],"metadata":{"id":"iQdIv24UoBl1"}},{"cell_type":"markdown","source":["#ê³¼ì œ 2 : make_fooling_image í•¨ìˆ˜ ì§œê¸°"],"metadata":{"id":"pnBiMhTrxUL3"}},{"cell_type":"code","source":["def make_fooling_image(X, target_y, model):\n","    \"\"\"\n","    Generate a fooling image that is close to X, but that the model classifies\n","    as target_y.\n","\n","    Inputs:\n","    - X: Input image; Tensor of shape (1, 3, 224, 224)\n","    - target_y: An integer in the range [0, 1000)\n","    - model: A pretrained CNN\n","\n","    Returns:\n","    - X_fooling: An image that is close to X, but that is classifed as target_y\n","    by the model.\n","    \"\"\"\n","    # ë„¤íŠ¸ì›Œí¬ë¥¼ ì†ì¼ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n","    X_fooling = X.clone()\n","    X_fooling = X_fooling.requires_grad_()\n","\n","    learning_rate = 1\n","\n","    ##############################################################################\n","    # TODO:\n","    # 1.ë°˜ë³µë¬¸ì„ ì‘ì„±í•˜ì—¬ target class scoreì— ëŒ€í•œ gradient ascentë¥¼ ì‚¬ìš©í•´ X_foolingì„ updateí•©ë‹ˆë‹¤\n","    # 2.Networkê°€ X_foolingì„ target_yë¡œ ë¶„ë¥˜í• ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤\n","\n","\n","    #ë¹ˆì¹¸ ì‘ì„± 5ê°€ì§€, ì•„ë˜ì˜ #######ë¶€ë¶„ì„ ì±„ì›Œì£¼ì„¸ìš”\n","    ##############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    while True:\n","\n","        #1. ëª¨ë¸ì˜ outputì„ ì–»ìœ¼ì„¸ìš”\n","\n","        pred_y = model(X_fooling)\n","\n","        #2. ëª¨ë¸ì˜ outputì´ target classì´ë¼ë©´?\n","\n","        index = torch.argmax(pred_y, dim=1)\n","\n","        if index[0] == target_y:\n","          break\n","\n","        #3. ì—¬ê¸°ì„œì˜ LossëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ìš°ë¦¬ê°€ ë¬´ì—‡ì„ í•˜ê¸° ìœ„í•´ fooling imageë¥¼ ë§Œë“¤ì—ˆë‚˜ìš”?\n","\n","        loss = pred_y[0, target_y]\n","\n","        #4. Backpropagation\n","\n","        loss.backward()\n","\n","        grad = X_fooling.grad.data\n","\n","        #5.Gradient ascentë¥¼ í†µí•´ image updateë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n","        #Image updateë¥¼ í•˜ê³ ë‚˜ì„œëŠ” ë°˜ë“œì‹œ ì´ê²ƒì„ í•´ì¤˜ì•¼í•©ë‹ˆë‹¤. ê·¸ë˜ì•¼ ë°˜ë³µí•´ì„œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê²Ÿì£ ?\n","        #Hint: dX = learning_rate * g / ||g||_2. (L2 normì„ ì ìš©í•œ normliazed graient stepì„ ì‚¬ìš©í•©ë‹ˆë‹¤)\n","\n","        X_fooling.data += learning_rate * (grad / grad.norm())\n","\n","        X_fooling.grad.zero_()\n","\n","    ##############################################################################\n","    #                             END OF YOUR CODE                               #\n","    ##############################################################################\n","    return X_fooling"],"metadata":{"id":"vDJkBuyBLSSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWiWI45B9DN9"},"outputs":[],"source":["idx = 0\n","target_y = 6\n","\n","X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","X_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\n","\n","scores = model(X_fooling)\n","assert target_y == scores.data.max(1)[1][0].item(), 'The model is not fooled!'"]},{"cell_type":"markdown","metadata":{"id":"nyt3QIXB9DN-"},"source":["Fooling imageë¥¼ ë§Œë“¤ì–´ë³´ê³  original imageì™€ ë¹„êµí•´ë³´ì„¸ìš”!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0uHcVua9DN_"},"outputs":[],"source":["X_fooling_np = deprocess(X_fooling.clone())\n","X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n","\n","plt.subplot(1, 4, 1)\n","plt.imshow(X[idx])\n","plt.title(class_names[y[idx]])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 2)\n","plt.imshow(X_fooling_np)\n","plt.title(class_names[target_y])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 3)\n","X_pre = preprocess(Image.fromarray(X[idx]))\n","diff = np.asarray(deprocess(X_fooling - X_pre, should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Difference')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 4)\n","diff = np.asarray(deprocess(10 * (X_fooling - X_pre), should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Magnified difference (10x)')\n","plt.axis('off')\n","\n","plt.gcf().set_size_inches(12, 5)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"SGie01E39DOA"},"source":["# Class Visualization : íŠ¹ì • í´ë˜ìŠ¤ì˜ classification scoreë¥¼ ìµœëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ë¥¼ í•©ì„±í•´ë³´ì\n","\n","\n","1. Class Visualizationì€ ëœë¤ ë…¸ì´ì¦ˆ ì´ë¯¸ì§€ë¥¼ ì‹œì‘ìœ¼ë¡œ target classì— ëŒ€í•´ gradient ascentë¥¼ ìˆ˜í–‰í•˜ì—¬, networkê°€ target classë¡œ ì¸ì‹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n","2. Several Regularization techniqueê³¼ í•¨ê»˜ ì“°ì—¬ generated imageë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì´ë„ë¡ í•©ë‹ˆë‹¤.\n","3. $s_y(I)$ : CNNì´ image $I$ë¥¼ class $y$ë¼ê³  classificationí•˜ëŠ” score (before softmax)\n","4. $R$ : L2 Regularizer\n","$$\n","I^* = \\arg\\max_I (s_y(I) - R(I))\n","$$\n","\n","$$\n","R(I) = \\lambda \\|I\\|_2^2\n","$$\n","\n","5. ìœ„ì˜ Optimization problemì„ gradient ascentë¡œ í•´ê²°í•˜ëŠ” ê²ƒ\n","\n","\n","[2] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. \"Deep Inside Convolutional Networks: Visualising\n","Image Classification Models and Saliency Maps\", ICLR Workshop 2014.\n","\n","[3] Yosinski et al, \"Understanding Neural Networks Through Deep Visualization\", ICML 2015 Deep Learning Workshop"]},{"cell_type":"markdown","source":["#ê³¼ì œ 3 : class_visualization_update_step í•¨ìˆ˜ ì§œê¸°"],"metadata":{"id":"i7NKt5dc5n2u"}},{"cell_type":"code","source":["def make_fooling_image(X, target_y, model):\n","\n","    # ë„¤íŠ¸ì›Œí¬ë¥¼ ì†ì¼ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n","    X_fooling = X.clone()\n","    X_fooling = X_fooling.requires_grad_()\n","\n","    learning_rate = 1\n","\n","    while True:\n","        #1. ëª¨ë¸ì˜ outputì„ ì–»ìœ¼ì„¸ìš”\n","\n","        pred_y = model(X_fooling)\n","\n","        #2. ëª¨ë¸ì˜ outputì´ target classì´ë¼ë©´?\n","\n","        index = torch.argmax(pred_y, dim=1)\n","\n","        if index[0] == target_y:\n","          break\n","\n","        #3. ì—¬ê¸°ì„œì˜ LossëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ìš°ë¦¬ê°€ ë¬´ì—‡ì„ í•˜ê¸° ìœ„í•´ fooling imageë¥¼ ë§Œë“¤ì—ˆë‚˜ìš”?\n","\n","        loss = pred_y[0, target_y]\n","\n","        #4. Backpropagation\n","\n","        loss.backward()\n","\n","        grad = X_fooling.grad.data\n","\n","        #5.Gradient ascentë¥¼ í†µí•´ image updateë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n","        #Image updateë¥¼ í•˜ê³ ë‚˜ì„œëŠ” ë°˜ë“œì‹œ ì´ê²ƒì„ í•´ì¤˜ì•¼í•©ë‹ˆë‹¤. ê·¸ë˜ì•¼ ë°˜ë³µí•´ì„œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ê²Ÿì£ ?\n","        #Hint: dX = learning_rate * g / ||g||_2. (L2 normì„ ì ìš©í•œ normliazed graient stepì„ ì‚¬ìš©í•©ë‹ˆë‹¤)\n","\n","        X_fooling.data += learning_rate * (grad / grad.norm())\n","\n","        X_fooling.grad.zero_()\n","\n","    return X_fooling"],"metadata":{"id":"MCSN8IEnuJGp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def class_visualization_update_step(img, model, target_y, l2_reg, learning_rate):\n","    ########################################################################\n","    #TODO:\n","    #1.target_y class scoreì— ëŒ€í•œ image í”½ì…€ì˜ gradient ê³„ì‚°\n","    #2.Gradient ascent ìˆ˜í–‰ with regularization\n","\n","\n","    #ê³¼ì œ 2ì™€ ê³¼ì •ì´ ê±°ì˜ ìœ ì‚¬í•˜ë¯€ë¡œ ìœ„ì˜ 2ê°€ì§€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ì½”ë“œë¥¼ ììœ ë¡­ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”\n","    #Regularizationì´ ë“¤ì–´ê°€ë¯€ë¡œ ë¶€í˜¸ì— ì£¼ì˜í•˜ì„¸ìš”!\n","    ########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    pred_y = model(img)\n","\n","    loss = pred_y[:, target_y]- (l2_reg * torch.square(torch.norm(img)))\n","\n","    loss.backward()\n","\n","    img.data += learning_rate * img.grad.data\n","\n","    img.grad.data.zero_()\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ########################################################################\n","    #                             END OF YOUR CODE                         #\n","    ########################################################################"],"metadata":{"id":"1Bgy1tv02IWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4QrLbvB9DOB"},"outputs":[],"source":["def create_class_visualization(target_y, model, dtype, **kwargs):\n","    \"\"\"\n","    Generate an image to maximize the score of target_y under a pretrained model.\n","\n","    Inputs:\n","    - target_y: Integer in the range [0, 1000) giving the index of the class\n","    - model: A pretrained CNN that will be used to generate the image\n","    - dtype: Torch datatype to use for computations\n","\n","    Keyword arguments:\n","    - l2_reg: Strength of L2 regularization on the image\n","    - learning_rate: How big of a step to take\n","    - num_iterations: How many iterations to use\n","    - blur_every: How often to blur the image as an implicit regularizer\n","    - max_jitter: How much to gjitter the image as an implicit regularizer\n","    - show_every: How often to show the intermediate result\n","    \"\"\"\n","    model.type(dtype)\n","    l2_reg = kwargs.pop('l2_reg', 1e-3)\n","    learning_rate = kwargs.pop('learning_rate', 25)\n","    num_iterations = kwargs.pop('num_iterations', 100)\n","    blur_every = kwargs.pop('blur_every', 10)\n","    max_jitter = kwargs.pop('max_jitter', 16)\n","    show_every = kwargs.pop('show_every', 25)\n","\n","    # Randomly initialize the image as a PyTorch Tensor, and make it requires gradient.\n","    img = torch.randn(1, 3, 224, 224).mul_(1.0).type(dtype).requires_grad_()\n","\n","    for t in range(num_iterations):\n","        # Randomly jitter the image a bit; this gives slightly nicer results\n","        ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter)\n","        img.data.copy_(jitter(img.data, ox, oy))\n","        class_visualization_update_step(img, model, target_y, l2_reg, learning_rate)\n","        # Undo the random jitter\n","        img.data.copy_(jitter(img.data, -ox, -oy))\n","\n","        # As regularizer, clamp and periodically blur the image\n","        for c in range(3):\n","            lo = float(-SQUEEZENET_MEAN[c] / SQUEEZENET_STD[c])\n","            hi = float((1.0 - SQUEEZENET_MEAN[c]) / SQUEEZENET_STD[c])\n","            img.data[:, c].clamp_(min=lo, max=hi)\n","        if t % blur_every == 0:\n","            blur_image(img.data, sigma=0.5)\n","\n","        # Periodically show the image\n","        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n","            plt.imshow(deprocess(img.data.clone().cpu()))\n","            class_name = class_names[target_y]\n","            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n","            plt.gcf().set_size_inches(4, 4)\n","            plt.axis('off')\n","            plt.show()\n","\n","    return deprocess(img.data.cpu())"]},{"cell_type":"markdown","source":["ì‹¤ì œ ì›í•˜ëŠ” target classì˜ ì´ë¯¸ì§€ê°€ ë§Œë“¤ì–´ì§€ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ì„¸ìš”"],"metadata":{"id":"TAgB6sLo5Un2"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"r1y60fQ39DOC"},"outputs":[],"source":["dtype = torch.FloatTensor\n","model.type(dtype)\n","\n","target_y = 76 # Tarantula\n","# target_y = 78 # Tick\n","# target_y = 187 # Yorkshire Terrier\n","# target_y = 683 # Oboe\n","# target_y = 366 # Gorilla\n","# target_y = 604 # Hourglass\n","out = create_class_visualization(target_y, model, dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7_Q9r7i9DOE"},"outputs":[],"source":["# target_y = 78 # Tick\n","# target_y = 187 # Yorkshire Terrier\n","# target_y = 683 # Oboe\n","# target_y = 366 # Gorilla\n","# target_y = 604 # Hourglass\n","\n","target_y = np.random.randint(1000)\n","print(class_names[target_y])\n","X = create_class_visualization(target_y, model, dtype)"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}