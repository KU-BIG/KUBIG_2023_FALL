import streamlit as st
from bs4 import BeautifulSoup
from utils import *
from keybert import KeyBERT
from transformers import BertModel
import openai


def generate_response(prompt):
    messages = []
    messages.append({"role": "user", "content": prompt})
    print("\ngenerate start.")
    completion = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0.7,
    )
    print("finished.")
    response = completion.choices[0].message.content
    return response


def transfer_text_style(texts, target_style, API, headers, max_length):
    flag = True
    print("\në¬¸ì²´ë¥¼ ë³€í™˜í•  í…ìŠ¤íŠ¸:", texts)
    while flag:
        msg = []
        for text in texts:
            inputs = f"{target_style} ë§íˆ¬ë¡œ ë³€í™˜:{text[0]}"
            msg.append(query(API, headers, {"inputs": inputs}, max_length))
        print("\në³€í™˜ ì™„ë£Œ: ", msg)
        try:
            if "error" in msg[0][0].keys():
                print("ë³€í™˜ ì˜¤ë¥˜. ë‹¤ì‹œ ë³€í™˜")
            else:
                flag = False
        except:
            if "error" in msg[0].keys():
                print("ë³€í™˜ ì˜¤ë¥˜. ë‹¤ì‹œ ë³€í™˜")
            else:
                flag = False
    return msg


# side bar
st.sidebar.title("ê³ ê¸‰ ê¸°ëŠ¥ğŸŒ¸")
select_species = st.sidebar.selectbox("ì–´ë–¤ ë§íˆ¬ë¡œ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?", ["ì›ë¬¸", "êµ¬ì–´ì²´", "ë‚˜ë£¨í† ", "enfp"])
st.sidebar.markdown("---")
slider_range = st.sidebar.slider("ì¶”ê°€ ì§ˆë¬¸ì€ ì–¼ë§ˆë‚˜ í• ê¹Œìš”?", 0.0, 1.0, (0.5))  # ì‹œì‘ ê°’  # ë ê°’
st.sidebar.markdown("---")

st.header("ë§ë‘ë‰´ìŠ¤ ğŸ§ ")
st.markdown("ë‰´ìŠ¤ë¥¼ í’€ì–´ì„œ ì„¤ëª…í•´ë“œë ¤ìš”.")

# ì‚¬ìš©ìë¡œë¶€í„° URL ì…ë ¥ ë°›ê¸°
with st.form("form", clear_on_submit=False):
    print("start")
    url = st.text_input("URL: ", "", key="input_url")
    submitted = st.form_submit_button("Submit")

if submitted:
    try:
        with st.spinner("### APIë¥¼ í˜¸ì¶œí•˜ëŠ” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ğŸ¤”"):
            API_TOKEN = "INSERT YOUR HUGGINGFACE API KEY"
            API_KEY = "INSERT YOUR OPENAI AIP KEY"
            headers = {"Authorization": f"Bearer {API_TOKEN}"}
            # kobart
            API_URL_kobart = (
                "https://api-inference.huggingface.co/models/ainize/kobart-news"
            )
            # koalpaca
            API_URL_koalpaca = "https://api-inference.huggingface.co/models/beomi/KoAlpaca-Polyglot-12.8B"
            # keybert
            model = BertModel.from_pretrained("skt/kobert-base-v1")
            kw_model = KeyBERT(model)
            # korean smilestyle dataset
            API_URL_korean = "https://api-inference.huggingface.co/models/heegyu/kobart-text-style-transfer"
            # gpt
            openai.api_key = API_KEY
        with st.spinner("### ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ğŸ¤”"):
            # beutifulsoup4ë¡œ url ì ‘ì† ë° í…ìŠ¤íŠ¸ í¬ë¡¤ë§
            response = requests.get(url)
            html_content = response.text
            soup = BeautifulSoup(html_content, "html.parser")
            title = soup.find("h2", class_="media_end_head_headline")  # ì œëª©
            writer = soup.find("em", class_="media_end_head_journalist_name")  # ê¸°ì
            datestamp = soup.find(
                "div", class_="media_end_head_info_datestamp"
            )  # ì…ë ¥ ìˆ˜ì •
            article = soup.find("article", class_="go_trans _article_content")  # ê¸°ì‚¬ ë³¸ë¬¸
            for br in article.find_all("br"):
                br.replace_with("\n")
            article_text = article.get_text()
            title = title.get_text()
            # <article> íƒœê·¸ ë‚´ì˜ ë‚´ìš© ì¤‘ <em> íƒœê·¸(ì£¼ì„) ì œì™¸
            exclude_em = article.find_all("em")
            for em_tag in exclude_em:
                article_text = article_text.replace(em_tag.get_text(), " ")

            # <article> íƒœê·¸ ë‚´ì˜ ë‚´ìš© ì¤‘ <strong> íƒœê·¸(ê°•ì¡° ë°˜ë³µ) ì œì™¸
            exclude_strong = article.find_all("strong")
            for strong_tag in exclude_strong:
                article_text = article_text.replace(strong_tag.get_text(), " ")

            text = preprocessing_text(article_text)  # ëª¨ë¸ í•™ìŠµê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬
            text = list2str(text)  # stringìœ¼ë¡œ ë³€í™˜
        try:
            with st.spinner("### ë‰´ìŠ¤ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ğŸ¤”"):
                output = query(API_URL_kobart, headers, {"inputs": text})  # ì¶”ì¶œ ìš”ì•½
                summary = output[0]["summary_text"]
            with st.spinner("### ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ğŸ¤”"):
                if not select_species == "ì›ë¬¸":
                    ms = transfer_text_style(
                        [[summary]],
                        select_species,
                        API_URL_korean,
                        headers,
                        max_length=1024,
                    )
                    summary = ms[0][0]["generated_text"]
                nouns = noun_extractor(summary)  # ëª…ì‚¬ ì¶”ì¶œ
                preprocessed_text = " ".join(nouns)
                keywords_items = kw_model.extract_keywords(
                    preprocessed_text,
                    keyphrase_ngram_range=(1, 1),
                    stop_words=None,
                    top_n=10,
                )  # í‚¤ì›Œë“œ ì¶”ì¶œ
                question_word = []
                for i, keyword in enumerate(keywords_items):
                    if i == 0:
                        keywords = keyword[0]
                    else:
                        keywords += ", " + keyword[0]
                    if keyword[1] > 1 - slider_range:
                        question_word.append(keyword[0])
                que, real_que = question_query(question_word)
            with st.spinner("### ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”... ğŸ¤”"):
                print("\në‹µë³€ì„ ìƒì„±í•  í‚¤ì›Œë“œ:", question_word)
                real_answer = generate_response(text + real_que)
                print("GPTê°€ ìƒì„±í•œ ë‹µë³€:", real_answer)
                real_answers = []
                if "(Q)" in real_answer:
                    for qna in real_answer.split("(Q)")[1 : 1 + len(question_word)]:
                        real_answers.append(qna.split("(A) ")[1:])
                else:
                    for qna in real_answer.split("(A) ")[1 : 1 + len(question_word)]:
                        real_answers.append([qna])
                print("\nSplitì„ ì ìš©í•œ ë‹µë³€:", real_answers)
                if not select_species == "ì›ë¬¸":
                    msg = transfer_text_style(
                        real_answers,
                        select_species,
                        API_URL_korean,
                        headers,
                        max_length=1024,
                    )
                    real_answers = []
                    print(msg)
                    for ms in msg:
                        real_answers.append(ms[0]["generated_text"])
                    real_answers = [real_answers]
                print("ë¬¸ì²´ ë³€í™˜ ì™„ë£Œ: ", real_answers)
                datetime = ""
                p_datetime = preprocessing_text(datestamp.get_text())  # datestampë„ ì „ì²˜ë¦¬
                for pdt in p_datetime:
                    if "ì…ë ¥" in pdt or "ìˆ˜ì •" in pdt:
                        datetime += pdt + " "
                wrt = " " if not writer else writer.get_text()
                st.write(f"#### {title}")
                st.markdown(
                    f'<p style="color: grey; font-size: 12px;">ì›ë¬¸: {datetime} {wrt} <br>ë§ë‘ë‰´ìŠ¤ì— ì˜í•´ ìš”ì•½ëœ ë‰´ìŠ¤ì…ë‹ˆë‹¤.</p>',
                    unsafe_allow_html=True,
                )

                st.write(summary)
                print("ì œëª© ë° ìš”ì•½ë¬¸ ì¶œë ¥ ì™„ë£Œ")
                for i in range(len(que)):
                    st.write(f"##### {que[i]}")
                    if not select_species == "ì›ë¬¸":
                        st.write(f"ğŸ§  {postprocessing_text(real_answers[0][i])}")
                    else:
                        st.write(f"ğŸ§  {postprocessing_text(real_answers[i][0])}")
                print("ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶œë ¥ ì™„ë£Œ")
                # st.write(keywords_items)
                st.markdown("")
                st.markdown("")
                st.markdown("")
                st.markdown(
                    f'<p style="color: grey; font-size: 12px;">ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}<br>ì¶”ì¶œëœ í‚¤ì›Œë“œì— ëŒ€í•´ ì„¤ëª…ì´ í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€ ì§ˆë¬¸ ì •ë„ë¥¼ ë†’ì—¬ë³´ì„¸ìš”!</p>',
                    unsafe_allow_html=True,
                )
                st.balloons()
        except:
            st.write("#### ì•„ì§ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì´ì—ìš”.")
            st.write("#### ì ì‹œ ë’¤ì— URLì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ˜­")
    except:
        st.write("#### ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”.")
        st.write("#### URLì„ ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”. ğŸ™„")
else:
    pass
