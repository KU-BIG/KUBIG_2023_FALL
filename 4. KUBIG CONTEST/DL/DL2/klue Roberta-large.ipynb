{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4088838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11921968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AdamW, RobertaForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b438848",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_data.csv\") # 초기 자료\n",
    "back_train = pd.read_csv(\"train_final1.csv\") # 역번역 자료\n",
    "test = pd.read_csv(\"test_data.csv\") # test 초기 자료\n",
    "back_test = pd.read_csv(\"test_final1.csv\") # 역번역 자료\n",
    "topic_dict=pd.read_csv(\"topic_dict.csv\")\n",
    "sample_submission=pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d42df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame()\n",
    "a[\"title\"]=back_train[\"title_kor\"]\n",
    "a[\"index\"]=back_train[\"index\"]\n",
    "a[\"topic_idx\"]=back_train[\"topic_idx\"]\n",
    "b=pd.DataFrame()\n",
    "b[\"title\"]=train[\"title\"]\n",
    "b[\"index\"]=train[\"index\"]\n",
    "b[\"topic_idx\"]=train[\"topic_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "377ea2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2=pd.concat([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc13ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(train2, test_size=0.2, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f918fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTDataset(Dataset):\n",
    "  \n",
    "  def __init__(self, csv_file):\n",
    "    self.dataset = csv_file\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "    print(self.dataset.describe())\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    row = self.dataset.iloc[idx, 1:3].values\n",
    "    text = row[0]\n",
    "    y = row[1]\n",
    "    inputs = self.tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=14,\n",
    "        pad_to_max_length=True,\n",
    "        add_special_tokens=True\n",
    "        )\n",
    "    \n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "    return input_ids, attention_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e76390fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTDataset_test(Dataset):\n",
    "  \n",
    "  def __init__(self, csv_file):\n",
    "    self.dataset = csv_file\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "\n",
    "    print(self.dataset.describe())\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    row = self.dataset.iloc[idx, 1:2].values\n",
    "    text = row[0]\n",
    "    inputs = self.tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=14,\n",
    "        pad_to_max_length=True,\n",
    "        add_special_tokens=True\n",
    "        )\n",
    "    \n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8bcff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              index     topic_idx\n",
      "count  73046.000000  73046.000000\n",
      "mean   22798.926539      3.160146\n",
      "std    13168.665730      1.933066\n",
      "min        0.000000      0.000000\n",
      "25%    11389.000000      2.000000\n",
      "50%    22776.000000      3.000000\n",
      "75%    34166.750000      5.000000\n",
      "max    45653.000000      6.000000\n",
      "              index     topic_idx\n",
      "count  18262.000000  18262.000000\n",
      "mean   22936.790822      3.177527\n",
      "std    13221.271437      1.932156\n",
      "min        2.000000      0.000000\n",
      "25%    11503.500000      2.000000\n",
      "50%    23008.500000      3.000000\n",
      "75%    34477.750000      5.000000\n",
      "max    45651.000000      6.000000\n",
      "              index\n",
      "count   9131.000000\n",
      "mean   50219.000000\n",
      "std     2636.036988\n",
      "min    45654.000000\n",
      "25%    47936.500000\n",
      "50%    50219.000000\n",
      "75%    52501.500000\n",
      "max    54784.000000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NTDataset(train)\n",
    "val_dataset = NTDataset(val)\n",
    "\n",
    "test_dataset = NTDataset_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca9d6054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device=torch.device('cuda') #relatively fast\n",
    "else:\n",
    "  device=torch.device('cpu') #only cpu for training & evaluating #Very slow\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf386ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-large\", num_labels=7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877ef787",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cff6f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wnsgu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946c9d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32286ea6193a4d7ebb7384e3fdc0d79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wnsgu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.003900176573401291 Accuracy: tensor(0.8335)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c4e4ca8fcb4837965437d5c123f183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "losses = []\n",
    "accuracies = []\n",
    "total_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    loss = F.cross_entropy(y_pred, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    _, predicted = torch.max(y_pred, 1)\n",
    "    correct += (predicted == y_batch).sum()\n",
    "    total += len(y_batch)\n",
    "  \n",
    "  losses.append(total_loss)\n",
    "  accuracies.append(correct.float() / total)\n",
    "  print(\"Train Loss:\", total_loss / total, \"Accuracy:\", correct.float() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(val_loader):\n",
    "  y_batch = y_batch.to(device)\n",
    "  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "  _, predicted = torch.max(y_pred, 1)\n",
    "  pred.append(predicted)\n",
    "  correct += (predicted == y_batch).sum()\n",
    "  total += len(y_batch)\n",
    "\n",
    "print(\"val accuracy:\", correct.float() / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9134a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model.eval()\n",
    "\n",
    "pred = []\n",
    "\n",
    "for input_ids_batch, attention_masks_batch in tqdm(test_loader):\n",
    "  y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "  _, predicted = torch.max(y_pred, 1)\n",
    "  pred.extend(predicted.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f751700",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.topic_idx = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7987c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"roberta_large.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
